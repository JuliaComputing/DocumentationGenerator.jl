<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Home · DecisionTree.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>DecisionTree.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li class="current"><a class="toctext" href>Home</a><ul class="internal"></ul></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Home</a></li></ul></nav><hr/><div id="topbar"><span>Home</span><a class="fa fa-bars" href="#"></a></div></header><p>Package doesn&#39;t contain Documenter docs.</p><p>Docs automatically generated by juliadocs.org</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="DecisionTree.AdaBoostStumpClassifier" href="#DecisionTree.AdaBoostStumpClassifier"><code>DecisionTree.AdaBoostStumpClassifier</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">AdaBoostStumpClassifier(; n_iterations::Int=0)</code></pre><p>Adaboosted decision tree stumps. See <a href="https://github.com/bensadeghi/DecisionTree.jl">DecisionTree.jl&#39;s documentation</a></p><p>Hyperparameters:</p><ul><li><code>n_iterations</code>: number of iterations of AdaBoost</li><li><code>rng</code>: the random number generator to use. Can be an <code>Int</code>, which will be used to seed and create a new random number generator.</li></ul><p>Implements <code>fit!</code>, <code>predict</code>, <code>predict_proba</code>, <code>get_classes</code></p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="DecisionTree.DecisionTreeClassifier" href="#DecisionTree.DecisionTreeClassifier"><code>DecisionTree.DecisionTreeClassifier</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">DecisionTreeClassifier(; pruning_purity_threshold=0.0,
                       max_depth::Int=-1,
                       min_samples_leaf::Int=1,
                       min_samples_split::Int=2,
                       min_purity_increase::Float=0.0,
                       n_subfeatures::Int=0,
                       rng=Random.GLOBAL_RNG)</code></pre><p>Decision tree classifier. See <a href="https://github.com/bensadeghi/DecisionTree.jl">DecisionTree.jl&#39;s documentation</a></p><p>Hyperparameters:</p><ul><li><code>pruning_purity_threshold</code>: (post-pruning) merge leaves having <code>&gt;=thresh</code> combined purity (default: no pruning)</li><li><code>max_depth</code>: maximum depth of the decision tree (default: no maximum)</li><li><code>min_samples_leaf</code>: the minimum number of samples each leaf needs to have (default: 1)</li><li><code>min_samples_split</code>: the minimum number of samples in needed for a split (default: 2)</li><li><code>min_purity_increase</code>: minimum purity needed for a split (default: 0.0)</li><li><code>n_subfeatures</code>: number of features to select at random (default: keep all)</li><li><code>rng</code>: the random number generator to use. Can be an <code>Int</code>, which will be used to seed and create a new random number generator.</li></ul><p>Implements <code>fit!</code>, <code>predict</code>, <code>predict_proba</code>, <code>get_classes</code></p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="DecisionTree.DecisionTreeRegressor" href="#DecisionTree.DecisionTreeRegressor"><code>DecisionTree.DecisionTreeRegressor</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">DecisionTreeRegressor(; pruning_purity_threshold=0.0,
                      max_depth::Int-1,
                      min_samples_leaf::Int=5,
                      min_samples_split::Int=2,
                      min_purity_increase::Float=0.0,
                      n_subfeatures::Int=0,
                      rng=Random.GLOBAL_RNG)</code></pre><p>Decision tree regression. See <a href="https://github.com/bensadeghi/DecisionTree.jl">DecisionTree.jl&#39;s documentation</a></p><p>Hyperparameters:</p><ul><li><code>pruning_purity_threshold</code>: (post-pruning) merge leaves having <code>&gt;=thresh</code> combined purity (default: no pruning)</li><li><code>max_depth</code>: maximum depth of the decision tree (default: no maximum)</li><li><code>min_samples_leaf</code>: the minimum number of samples each leaf needs to have (default: 5)</li><li><code>min_samples_split</code>: the minimum number of samples in needed for a split (default: 2)</li><li><code>min_purity_increase</code>: minimum purity needed for a split (default: 0.0)</li><li><code>n_subfeatures</code>: number of features to select at random (default: keep all)</li><li><code>rng</code>: the random number generator to use. Can be an <code>Int</code>, which will be used to seed and create a new random number generator.</li></ul><p>Implements <code>fit!</code>, <code>predict</code>, <code>get_classes</code></p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="DecisionTree.RandomForestClassifier" href="#DecisionTree.RandomForestClassifier"><code>DecisionTree.RandomForestClassifier</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">RandomForestClassifier(; n_subfeatures::Int=-1,
                       n_trees::Int=10,
                       partial_sampling::Float=0.7,
                       max_depth::Int=-1,
                       rng=Random.GLOBAL_RNG)</code></pre><p>Random forest classification. See <a href="https://github.com/bensadeghi/DecisionTree.jl">DecisionTree.jl&#39;s documentation</a></p><p>Hyperparameters:</p><ul><li><code>n_subfeatures</code>: number of features to consider at random per split (default: -1, sqrt(# features))</li><li><code>n_trees</code>: number of trees to train (default: 10)</li><li><code>partial_sampling</code>: fraction of samples to train each tree on (default: 0.7)</li><li><code>max_depth</code>: maximum depth of the decision trees (default: no maximum)</li><li><code>min_samples_leaf</code>: the minimum number of samples each leaf needs to have</li><li><code>min_samples_split</code>: the minimum number of samples in needed for a split</li><li><code>min_purity_increase</code>: minimum purity needed for a split</li><li><code>rng</code>: the random number generator to use. Can be an <code>Int</code>, which will be used to seed and create a new random number generator.</li></ul><p>Implements <code>fit!</code>, <code>predict</code>, <code>predict_proba</code>, <code>get_classes</code></p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="DecisionTree.RandomForestRegressor" href="#DecisionTree.RandomForestRegressor"><code>DecisionTree.RandomForestRegressor</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">RandomForestRegressor(; n_subfeatures::Int=-1,
                      n_trees::Int=10,
                      partial_sampling::Float=0.7,
                      max_depth::Int=-1,
                      min_samples_leaf::Int=5,
                      rng=Random.GLOBAL_RNG)</code></pre><p>Random forest regression. See <a href="https://github.com/bensadeghi/DecisionTree.jl">DecisionTree.jl&#39;s documentation</a></p><p>Hyperparameters:</p><ul><li><code>n_subfeatures</code>: number of features to consider at random per split (default: -1, sqrt(# features))</li><li><code>n_trees</code>: number of trees to train (default: 10)</li><li><code>partial_sampling</code>: fraction of samples to train each tree on (default: 0.7)</li><li><code>max_depth</code>: maximum depth of the decision trees (default: no maximum)</li><li><code>min_samples_leaf</code>: the minimum number of samples each leaf needs to have (default: 5)</li><li><code>min_samples_split</code>: the minimum number of samples in needed for a split</li><li><code>min_purity_increase</code>: minimum purity needed for a split</li><li><code>rng</code>: the random number generator to use. Can be an <code>Int</code>, which will be used to seed and create a new random number generator.</li></ul><p>Implements <code>fit!</code>, <code>predict</code>, <code>get_classes</code></p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="DecisionTree.apply_adaboost_stumps_proba-Union{Tuple{T}, Tuple{S}, Tuple{Ensemble{S,T},Array{Float64,1},Array{S,1},Array{T,1}}} where T where S" href="#DecisionTree.apply_adaboost_stumps_proba-Union{Tuple{T}, Tuple{S}, Tuple{Ensemble{S,T},Array{Float64,1},Array{S,1},Array{T,1}}} where T where S"><code>DecisionTree.apply_adaboost_stumps_proba</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">apply_adaboost_stumps_proba(stumps::Ensemble, coeffs, features, labels::Vector)</code></pre><p>computes P(L=label|X) for each row in <code>features</code>. It returns a <code>N_row x n_labels</code> matrix of probabilities, each row summing up to 1.</p><p><code>col_labels</code> is a vector containing the distinct labels (eg. [&quot;versicolor&quot;, &quot;virginica&quot;, &quot;setosa&quot;]). It specifies the column ordering of the output matrix. </p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="DecisionTree.apply_forest_proba-Union{Tuple{T}, Tuple{S}, Tuple{Ensemble{S,T},Array{S,1},Any}} where T where S" href="#DecisionTree.apply_forest_proba-Union{Tuple{T}, Tuple{S}, Tuple{Ensemble{S,T},Array{S,1},Any}} where T where S"><code>DecisionTree.apply_forest_proba</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">apply_forest_proba(forest::Ensemble, features, col_labels::Vector)</code></pre><p>computes P(L=label|X) for each row in <code>features</code>. It returns a <code>N_row x n_labels</code> matrix of probabilities, each row summing up to 1.</p><p><code>col_labels</code> is a vector containing the distinct labels (eg. [&quot;versicolor&quot;, &quot;virginica&quot;, &quot;setosa&quot;]). It specifies the column ordering of the output matrix. </p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="DecisionTree.apply_tree_proba-Union{Tuple{T}, Tuple{S}, Tuple{Leaf{T},Array{S,1},Any}} where T where S" href="#DecisionTree.apply_tree_proba-Union{Tuple{T}, Tuple{S}, Tuple{Leaf{T},Array{S,1},Any}} where T where S"><code>DecisionTree.apply_tree_proba</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">apply_tree_proba(::Node, features, col_labels::Vector)</code></pre><p>computes P(L=label|X) for each row in <code>features</code>. It returns a <code>N_row x n_labels</code> matrix of probabilities, each row summing up to 1.</p><p><code>col_labels</code> is a vector containing the distinct labels (eg. [&quot;versicolor&quot;, &quot;virginica&quot;, &quot;setosa&quot;]). It specifies the column ordering of the output matrix. </p></div></div></section><footer><hr/></footer></article></body></html>
