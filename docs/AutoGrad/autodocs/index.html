<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Docstrings · AutoGrad.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>AutoGrad.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Readme</a></li><li class="current"><a class="toctext" href>Docstrings</a><ul class="internal"></ul></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Docstrings</a></li></ul></nav><hr/><div id="topbar"><span>Docstrings</span><a class="fa fa-bars" href="#"></a></div></header><p>Package doesn&#39;t contain Documenter docs.</p><p>Docs automatically generated by juliadocs.org</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AutoGrad.Param" href="#AutoGrad.Param"><code>AutoGrad.Param</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>Usage:</p><pre><code class="language-none">x = Param([1,2,3])          # user declares parameters with `Param`
x =&gt; P([1,2,3])             # `Param` is just a struct wrapping a value
value(x) =&gt; [1,2,3]         # `value` returns the thing wrapped
sum(x .* x) =&gt; 14           # Params act like regular values
y = @diff sum(x .* x)       # Except when we differentiate using `@diff`
y =&gt; T(14)                  # you get another struct
value(y) =&gt; 14              # which carries the same result
params(y) =&gt; [x]            # and the Params that it depends on 
grad(y,x) =&gt; [2,4,6]        # and the gradients for all Params</code></pre><p><code>Param(x)</code> returns a struct that acts like <code>x</code> but marks it as a parameter you want to compute gradients with respect to.</p><p><code>@diff expr</code> evaluates an expression and returns a struct that contains the result (which should be a scalar) and gradient information.</p><p><code>grad(y, x)</code> returns the gradient of <code>y</code> (output by @diff) with respect to any parameter <code>x::Param</code>, or  <code>nothing</code> if the gradient is 0.</p><p><code>value(x)</code> returns the value associated with <code>x</code> if <code>x</code> is a <code>Param</code> or the output of <code>@diff</code>, otherwise returns <code>x</code>.</p><p><code>params(x)</code> returns an iterator of Params found by a recursive search of object <code>x</code>.</p><p>Alternative usage:</p><pre><code class="language-none">x = [1 2 3]
f(x) = sum(x .* x)
f(x) =&gt; 14
grad(f)(x) =&gt; [2 4 6]
gradloss(f)(x) =&gt; ([2 4 6], 14)</code></pre><p>Given a scalar valued function <code>f</code>, <code>grad(f,argnum=1)</code> returns another function <code>g</code> which takes the same inputs as <code>f</code> and returns the gradient of the output with respect to the argnum&#39;th argument. <code>gradloss</code> is similar except the resulting function also returns f&#39;s output.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AutoGrad.cat1d-Tuple" href="#AutoGrad.cat1d-Tuple"><code>AutoGrad.cat1d</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">cat1d(args...)</code></pre><p>Return <code>vcat(vec.(args)...)</code> but possibly more efficiently. Can be used to concatenate the contents of arrays with different shapes and sizes.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AutoGrad.grad" href="#AutoGrad.grad"><code>AutoGrad.grad</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>Usage:</p><pre><code class="language-none">x = Param([1,2,3])          # user declares parameters with `Param`
x =&gt; P([1,2,3])             # `Param` is just a struct wrapping a value
value(x) =&gt; [1,2,3]         # `value` returns the thing wrapped
sum(x .* x) =&gt; 14           # Params act like regular values
y = @diff sum(x .* x)       # Except when we differentiate using `@diff`
y =&gt; T(14)                  # you get another struct
value(y) =&gt; 14              # which carries the same result
params(y) =&gt; [x]            # and the Params that it depends on 
grad(y,x) =&gt; [2,4,6]        # and the gradients for all Params</code></pre><p><code>Param(x)</code> returns a struct that acts like <code>x</code> but marks it as a parameter you want to compute gradients with respect to.</p><p><code>@diff expr</code> evaluates an expression and returns a struct that contains the result (which should be a scalar) and gradient information.</p><p><code>grad(y, x)</code> returns the gradient of <code>y</code> (output by @diff) with respect to any parameter <code>x::Param</code>, or  <code>nothing</code> if the gradient is 0.</p><p><code>value(x)</code> returns the value associated with <code>x</code> if <code>x</code> is a <code>Param</code> or the output of <code>@diff</code>, otherwise returns <code>x</code>.</p><p><code>params(x)</code> returns an iterator of Params found by a recursive search of object <code>x</code>.</p><p>Alternative usage:</p><pre><code class="language-none">x = [1 2 3]
f(x) = sum(x .* x)
f(x) =&gt; 14
grad(f)(x) =&gt; [2 4 6]
gradloss(f)(x) =&gt; ([2 4 6], 14)</code></pre><p>Given a scalar valued function <code>f</code>, <code>grad(f,argnum=1)</code> returns another function <code>g</code> which takes the same inputs as <code>f</code> and returns the gradient of the output with respect to the argnum&#39;th argument. <code>gradloss</code> is similar except the resulting function also returns f&#39;s output.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AutoGrad.gradloss" href="#AutoGrad.gradloss"><code>AutoGrad.gradloss</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>Usage:</p><pre><code class="language-none">x = Param([1,2,3])          # user declares parameters with `Param`
x =&gt; P([1,2,3])             # `Param` is just a struct wrapping a value
value(x) =&gt; [1,2,3]         # `value` returns the thing wrapped
sum(x .* x) =&gt; 14           # Params act like regular values
y = @diff sum(x .* x)       # Except when we differentiate using `@diff`
y =&gt; T(14)                  # you get another struct
value(y) =&gt; 14              # which carries the same result
params(y) =&gt; [x]            # and the Params that it depends on 
grad(y,x) =&gt; [2,4,6]        # and the gradients for all Params</code></pre><p><code>Param(x)</code> returns a struct that acts like <code>x</code> but marks it as a parameter you want to compute gradients with respect to.</p><p><code>@diff expr</code> evaluates an expression and returns a struct that contains the result (which should be a scalar) and gradient information.</p><p><code>grad(y, x)</code> returns the gradient of <code>y</code> (output by @diff) with respect to any parameter <code>x::Param</code>, or  <code>nothing</code> if the gradient is 0.</p><p><code>value(x)</code> returns the value associated with <code>x</code> if <code>x</code> is a <code>Param</code> or the output of <code>@diff</code>, otherwise returns <code>x</code>.</p><p><code>params(x)</code> returns an iterator of Params found by a recursive search of object <code>x</code>.</p><p>Alternative usage:</p><pre><code class="language-none">x = [1 2 3]
f(x) = sum(x .* x)
f(x) =&gt; 14
grad(f)(x) =&gt; [2 4 6]
gradloss(f)(x) =&gt; ([2 4 6], 14)</code></pre><p>Given a scalar valued function <code>f</code>, <code>grad(f,argnum=1)</code> returns another function <code>g</code> which takes the same inputs as <code>f</code> and returns the gradient of the output with respect to the argnum&#39;th argument. <code>gradloss</code> is similar except the resulting function also returns f&#39;s output.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AutoGrad.params" href="#AutoGrad.params"><code>AutoGrad.params</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>Usage:</p><pre><code class="language-none">x = Param([1,2,3])          # user declares parameters with `Param`
x =&gt; P([1,2,3])             # `Param` is just a struct wrapping a value
value(x) =&gt; [1,2,3]         # `value` returns the thing wrapped
sum(x .* x) =&gt; 14           # Params act like regular values
y = @diff sum(x .* x)       # Except when we differentiate using `@diff`
y =&gt; T(14)                  # you get another struct
value(y) =&gt; 14              # which carries the same result
params(y) =&gt; [x]            # and the Params that it depends on 
grad(y,x) =&gt; [2,4,6]        # and the gradients for all Params</code></pre><p><code>Param(x)</code> returns a struct that acts like <code>x</code> but marks it as a parameter you want to compute gradients with respect to.</p><p><code>@diff expr</code> evaluates an expression and returns a struct that contains the result (which should be a scalar) and gradient information.</p><p><code>grad(y, x)</code> returns the gradient of <code>y</code> (output by @diff) with respect to any parameter <code>x::Param</code>, or  <code>nothing</code> if the gradient is 0.</p><p><code>value(x)</code> returns the value associated with <code>x</code> if <code>x</code> is a <code>Param</code> or the output of <code>@diff</code>, otherwise returns <code>x</code>.</p><p><code>params(x)</code> returns an iterator of Params found by a recursive search of object <code>x</code>.</p><p>Alternative usage:</p><pre><code class="language-none">x = [1 2 3]
f(x) = sum(x .* x)
f(x) =&gt; 14
grad(f)(x) =&gt; [2 4 6]
gradloss(f)(x) =&gt; ([2 4 6], 14)</code></pre><p>Given a scalar valued function <code>f</code>, <code>grad(f,argnum=1)</code> returns another function <code>g</code> which takes the same inputs as <code>f</code> and returns the gradient of the output with respect to the argnum&#39;th argument. <code>gradloss</code> is similar except the resulting function also returns f&#39;s output.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AutoGrad.value" href="#AutoGrad.value"><code>AutoGrad.value</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>Usage:</p><pre><code class="language-none">x = Param([1,2,3])          # user declares parameters with `Param`
x =&gt; P([1,2,3])             # `Param` is just a struct wrapping a value
value(x) =&gt; [1,2,3]         # `value` returns the thing wrapped
sum(x .* x) =&gt; 14           # Params act like regular values
y = @diff sum(x .* x)       # Except when we differentiate using `@diff`
y =&gt; T(14)                  # you get another struct
value(y) =&gt; 14              # which carries the same result
params(y) =&gt; [x]            # and the Params that it depends on 
grad(y,x) =&gt; [2,4,6]        # and the gradients for all Params</code></pre><p><code>Param(x)</code> returns a struct that acts like <code>x</code> but marks it as a parameter you want to compute gradients with respect to.</p><p><code>@diff expr</code> evaluates an expression and returns a struct that contains the result (which should be a scalar) and gradient information.</p><p><code>grad(y, x)</code> returns the gradient of <code>y</code> (output by @diff) with respect to any parameter <code>x::Param</code>, or  <code>nothing</code> if the gradient is 0.</p><p><code>value(x)</code> returns the value associated with <code>x</code> if <code>x</code> is a <code>Param</code> or the output of <code>@diff</code>, otherwise returns <code>x</code>.</p><p><code>params(x)</code> returns an iterator of Params found by a recursive search of object <code>x</code>.</p><p>Alternative usage:</p><pre><code class="language-none">x = [1 2 3]
f(x) = sum(x .* x)
f(x) =&gt; 14
grad(f)(x) =&gt; [2 4 6]
gradloss(f)(x) =&gt; ([2 4 6], 14)</code></pre><p>Given a scalar valued function <code>f</code>, <code>grad(f,argnum=1)</code> returns another function <code>g</code> which takes the same inputs as <code>f</code> and returns the gradient of the output with respect to the argnum&#39;th argument. <code>gradloss</code> is similar except the resulting function also returns f&#39;s output.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AutoGrad.dir-Tuple" href="#AutoGrad.dir-Tuple"><code>AutoGrad.dir</code></a> — <span class="docstring-category">Method</span>.</div><div><div><p>Use <code>AutoGrad.dir(path...)</code> to construct paths relative to AutoGrad root.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AutoGrad.gcheck" href="#AutoGrad.gcheck"><code>AutoGrad.gcheck</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">gcheck(f, x...; kw, o...)
@gcheck f(x...; kw...) (opt1=val1,opt2=val2,...)</code></pre><p>Numerically check the gradient of <code>f(x...; kw...)</code> and return a boolean result.</p><p>Example call: <code>gcheck(nll,model,x,y)</code> or <code>@gcheck nll(model,x,y)</code>. The parameters should be marked as <code>Param</code> arrays in <code>f</code>, <code>x</code>, and/or <code>kw</code>.  Only 10 random entries in each large numeric array are checked by default.  If the output of <code>f</code> is not a number, we check the gradient of <code>sum(f(x...; kw...))</code>. Keyword arguments:</p><ul><li><code>kw=()</code>: keyword arguments to be passed to <code>f</code>, i.e. <code>f(x...; kw...)</code></li><li><code>nsample=10</code>: number of random entries from each param to check</li><li><code>atol=0.01,rtol=0.05</code>: tolerance parameters.  See <code>isapprox</code> for their meaning.</li><li><code>delta=0.0001</code>: step size for numerical gradient calculation.</li><li><code>verbose=1</code>: 0 prints nothing, 1 shows failing tests, 2 shows all tests.</li></ul></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AutoGrad.gradcheck-Tuple{Any,Vararg{Any,N} where N}" href="#AutoGrad.gradcheck-Tuple{Any,Vararg{Any,N} where N}"><code>AutoGrad.gradcheck</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">gradcheck(f, x...; kwargs...)</code></pre><p>Numerically check the gradient of <code>f(x...)</code> and return a boolean result.</p><p>Each argument can be a Number, Array, Tuple or Dict which in turn can contain other Arrays etc.  Only 10 random entries in each large numeric array are checked by default.  If the output of <code>f</code> is not a number, we check the gradient of <code>sum(f(x...))</code>. See also <code>gcheck</code> for a different take on marking parameters.</p><p><strong>Keywords</strong></p><ul><li><p><code>args=:</code>: the argument indices to check gradients with respect to. Could be an array or range of indices or a single index. By default all arguments that have a <code>length</code> method are checked.</p></li><li><p><code>kw=()</code>: keyword arguments to be passed to <code>f</code>.</p></li><li><p><code>nsample=10</code>: number of random entries from each numeric array in gradient <code>dw=(grad(f))(w,x...;o...)</code> compared to their numerical estimates.</p></li><li><p><code>atol=rtol=0.01</code>: tolerance parameters.  See <code>isapprox</code> for their meaning.</p></li><li><p><code>delta=0.0001</code>: step size for numerical gradient calculation.</p></li><li><p><code>verbose=1</code>: 0 prints nothing, 1 shows failing tests, 2 shows all tests.</p></li></ul></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AutoGrad.randcheck" href="#AutoGrad.randcheck"><code>AutoGrad.randcheck</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>Test a numeric function with Float32/64 randn scalars and randn arrays, possibly transforming the input to match the domain</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AutoGrad.unbroadcast-Tuple{Any,Any}" href="#AutoGrad.unbroadcast-Tuple{Any,Any}"><code>AutoGrad.unbroadcast</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">unbroadcast(x,dx)</code></pre><p>Bring dx to x&#39;s size via unbroadcasting (reduction). This is needed when defining gradients of multi-argument broadcasting functions where the arguments and the result may be of different sizes.</p></div></div></section><footer><hr/><a class="previous" href="../"><span class="direction">Previous</span><span class="title">Readme</span></a></footer></article></body></html>
