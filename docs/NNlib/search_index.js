var documenterSearchIndex = {"docs": [

{
    "location": "#",
    "page": "Readme",
    "title": "Readme",
    "category": "page",
    "text": ""
},

{
    "location": "#NNlib-1",
    "page": "Readme",
    "title": "NNlib",
    "category": "section",
    "text": "(Image: Build Status) (Image: Build status)This package will provide a library of functions useful for ML, such as softmax, sigmoid, convolutions and pooling. It doesn\'t provide any other \"high-level\" functionality like layers or AD.Other packages can build on these functions as if they were defined in Base Julia; for example, CuArrays provides GPU kernels, and Flux provides automatic differentiation; both can work together without explicitly being aware of each other."
},

{
    "location": "autodocs/#NNlib.elu",
    "page": "Docstrings",
    "title": "NNlib.elu",
    "category": "function",
    "text": "elu(x, α = 1) =\n  x > 0 ? x : α * (exp(x) - 1)\n\nExponential Linear Unit activation function. See Fast and Accurate Deep Network Learning by Exponential Linear Units. You can also specify the coefficient explicitly, e.g. elu(x, 1).\n\n\n\n\n\n"
},

{
    "location": "autodocs/#NNlib.leakyrelu",
    "page": "Docstrings",
    "title": "NNlib.leakyrelu",
    "category": "function",
    "text": "leakyrelu(x) = max(0.01x, x)\n\nLeaky Rectified Linear Unit activation function. You can also specify the coefficient explicitly, e.g. leakyrelu(x, 0.01).\n\n\n\n\n\n"
},

{
    "location": "autodocs/#NNlib.logsoftmax",
    "page": "Docstrings",
    "title": "NNlib.logsoftmax",
    "category": "function",
    "text": "logsoftmax(xs) = log.(exp.(xs) ./ sum(exp.(xs)))\n\nlogsoftmax computes the log of softmax(xs) and it is more numerically stable than softmax function in computing the cross entropy loss.\n\n\n\n\n\n"
},

{
    "location": "autodocs/#NNlib.logσ-Tuple{Any}",
    "page": "Docstrings",
    "title": "NNlib.logσ",
    "category": "method",
    "text": "logσ(x)\n\nReturn log(σ(x)) which is computed in a numerically stable way.\n\njulia> logσ(0.)\n-0.6931471805599453\njulia> logσ.([-100, -10, 100.])\n3-element Array{Float64,1}:\n -100.0\n  -10.0\n   -0.0\n\n\n\n\n\n"
},

{
    "location": "autodocs/#NNlib.relu-Tuple{Any}",
    "page": "Docstrings",
    "title": "NNlib.relu",
    "category": "method",
    "text": "relu(x) = max(0, x)\n\nRectified Linear Unit activation function.\n\n\n\n\n\n"
},

{
    "location": "autodocs/#NNlib.selu-Tuple{Any}",
    "page": "Docstrings",
    "title": "NNlib.selu",
    "category": "method",
    "text": "selu(x) = λ * (x ≥ 0 ? x : α * (exp(x) - 1))\n\nλ ≈ 1.0507\nα ≈ 1.6733\n\nScaled exponential linear units. See Self-Normalizing Neural Networks.\n\n\n\n\n\n"
},

{
    "location": "autodocs/#NNlib.softmax",
    "page": "Docstrings",
    "title": "NNlib.softmax",
    "category": "function",
    "text": "softmax(xs) = exp.(xs) ./ sum(exp.(xs))\n\nSoftmax takes log-probabilities (any real vector) and returns a probability distribution that sums to 1.\n\nIf given a matrix it will treat it as a batch of vectors, with each column independent.\n\njulia> softmax([1,2,3.])\n3-element Array{Float64,1}:\n  0.0900306\n  0.244728\n  0.665241\n\n\n\n\n\n"
},

{
    "location": "autodocs/#NNlib.softplus-Tuple{Any}",
    "page": "Docstrings",
    "title": "NNlib.softplus",
    "category": "method",
    "text": "softplus(x) = log(exp(x) + 1)\n\nSee Deep Sparse Rectifier Neural Networks.\n\n\n\n\n\n"
},

{
    "location": "autodocs/#NNlib.softsign-Tuple{Any}",
    "page": "Docstrings",
    "title": "NNlib.softsign",
    "category": "method",
    "text": "softsign(x) = x / (1 + |x|)\n\nSee Quadratic Polynomials Learn Better Image Features.\n\n\n\n\n\n"
},

{
    "location": "autodocs/#NNlib.swish-Tuple{Any}",
    "page": "Docstrings",
    "title": "NNlib.swish",
    "category": "method",
    "text": "swish(x) = x * σ(x)\n\nSelf-gated actvation function. See Swish: a Self-Gated Activation Function.\n\n\n\n\n\n"
},

{
    "location": "autodocs/#NNlib.σ-Tuple{Any}",
    "page": "Docstrings",
    "title": "NNlib.σ",
    "category": "method",
    "text": "σ(x) = 1 / (1 + exp(-x))\n\nClassic sigmoid activation function.\n\n\n\n\n\n"
},

{
    "location": "autodocs/#NNlib.log_fast",
    "page": "Docstrings",
    "title": "NNlib.log_fast",
    "category": "function",
    "text": "log_fast(x)\n\nCompute the natural logarithm of x. The inverse of the natural logarithm is the natural expoenential function exp(x)\n\n\n\n\n\n"
},

{
    "location": "autodocs/#",
    "page": "Docstrings",
    "title": "Docstrings",
    "category": "page",
    "text": "Package doesn\'t contain Documenter docs.Docs automatically generated by juliadocs.orgModules = [NNlib]\nOrder = [:type, :function]"
},

]}
